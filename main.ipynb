{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No config specified, defaulting to: arxiv-summarization/section\n",
      "Found cached dataset arxiv-summarization (C:/Users/Justin Du/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3)\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'abstract'],\n",
       "    num_rows: 6440\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = dataset['train']\n",
    "data_test = dataset['test']\n",
    "data_val = dataset['validation']\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "checkpoint = 'facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "\n",
    "def tokenize_function(data):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        data[\"article\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    labels = tokenizer(\n",
    "        data['abstract'],\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/Justin Du/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3\\cache-aa89378e5dcd8e81.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'abstract', 'input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'],\n",
       "    num_rows: 6440\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = data_test.map(tokenize_function, batched=True)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = tok['input_ids'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1990, 59, 291, 107, 5, 936, 9, 3611, 9, 765, 111, 1385, 1022, 9, 4118, 1940, 34, 57, 1687, 18808, 479, 1437, 50118, 171, 3725, 8069, 5, 765, 111, 1385, 27185, 2192, 9, 5, 1337, 21383, 9, 4118, 1940, 479, 1437, 50118, 484, 27185, 2192, 58, 12333, 2156, 53, 5, 27185, 2192, 59, 18918, 360, 8, 31, 5, 22455, 9, 787, 1178, 40051, 246, 68, 27779, 360, 36, 787, 1178, 40051, 306, 68, 27779, 107, 4839, 32, 2801, 144, 747, 479, 1437, 50118, 78, 9, 106, 21, 2967, 30, 787, 45421, 1459, 11, 5, 37627, 2407, 15285, 731, 9, 42685, 111, 33803, 34968, 12333, 30, 5, 42685, 111, 33803, 19416, 5638, 5906, 10009, 5, 18134, 4118, 4532, 2511, 36, 5278, 119, 4839, 479, 1437, 50118, 42, 675, 24414, 21, 1474, 13, 97, 4118, 34968, 414, 8, 13, 5, 276, 86, 675, 787, 45421, 1459, 479, 1437, 50118, 24, 21, 67, 303, 11, 1759, 1054, 34968, 148, 4118, 16726, 753, 8, 291, 787, 45421, 1459, 2156, 53, 24, 21, 45, 303, 11, 5, 4118, 34968, 414, 148, 4118, 16726, 820, 787, 45421, 1459, 479, 1437, 50118, 18134, 1437, 1437, 1437, 484, 7241, 994, 1474, 1065, 775, 13, 5, 1230, 3778, 27840, 443, 414, 479, 787, 45421, 1459, 8069, 5, 3778, 27840, 414, 31, 504, 5243, 42080, 479, 1437, 50118, 79, 303, 5, 18918, 12, 1208, 675, 24414, 11, 414, 2189, 31, 1105, 107, 479, 1437, 50118, 42, 675, 24414, 16, 460, 26293, 13, 65, 9, 5, 4118, 23385, 19291, 29492, 36, 5, 3174, 32515, 13, 16726, 316, 996, 8, 5, 3285, 32515, 13, 16726, 545, 2146, 4839, 479, 1437, 50118, 41353, 2156, 24, 16, 129, 1455, 148, 43660, 29, 9, 4532, 1940, 36, 11, 7585, 9, 508, 107, 4839, 479, 1437, 50118, 1122, 12406, 1023, 1635, 17341, 5901, 10382, 995, 1409, 2055, 787, 45421, 1459, 479, 1437, 50118, 51, 5049, 5, 276, 476, 8576, 5448, 25, 11257, 2156, 53, 5, 1230, 3778, 27840, 443, 414, 36, 16726, 316, 2146, 4839, 58, 6408, 88, 158, 10941, 86, 651, 479, 1437, 50118, 5, 27185, 2192, 58, 10593, 13, 5, 13135, 22455, 4981, 15314, 295, 39569, 36, 727, 2619, 360, 4839, 8, 13, 349, 9, 158, 86, 651, 479, 1437, 50118, 5, 7601, 969, 14, 5, 675, 24414, 227, 379, 2663, 2466, 360, 16, 27697, 1233, 148, 70, 16726, 31, 545, 7, 733, 479, 1437, 50118, 5, 1687, 21414, 58, 2442, 542, 337, 10001, 71, 8201, 5, 365, 12, 180, 4943, 8, 9889, 5, 476, 8576, 1966, 479, 1437, 50118, 787, 45421, 1459, 341, 5, 4605, 2716, 9205, 13, 5, 1230, 3778, 27840, 911, 227, 504, 5243, 8, 9095, 479, 1437, 50118, 51, 3030, 5, 43660, 29, 9, 2772, 9, 42, 675, 24414, 8, 4633, 14, 24, 6822, 198, 5, 4532, 1940, 675, 11, 16726, 545, 7, 733, 479, 1437, 50118, 41353, 2156, 5, 476, 9, 42, 675, 24414, 554, 1197, 23, 4943, 753, 2156, 8065, 11, 16726, 291, 8, 733, 8, 49053, 3215, 71, 4943, 733, 479, 1437, 50118, 1122, 260, 44965, 17341, 25870, 196, 1409, 2055, 787, 45421, 1459, 2156, 53, 13, 3778, 27840, 346, 2156, 4118, 2508, 29051, 2156, 3222, 11181, 33267, 17512, 882, 8, 5473, 1075, 44650, 1940, 1965, 787, 1178, 40051, 245, 479, 1437, 50118, 148, 17616, 111, 3788, 5, 3778, 27840, 346, 4605, 2716, 476, 9, 5788, 540, 87, 65, 76, 924, 10, 13030, 636, 10795, 19, 5, 4359, 9, 5, 4118, 4943, 4, 627, 25502, 12, 1208, 675, 16, 5395, 8, 63, 34812, 212, 16, 3651, 198, 5, 12910, 111, 11724, 22455, 11, 818, 70, 4118, 2508, 17294, 479, 1437, 50118, 5, 8066, 9, 5, 25664, 12, 1208, 675, 24414, 11, 3778, 27840, 414, 58, 1474, 30, 787, 45421, 1459, 479, 1437, 50118, 51, 1687, 5, 678, 9355, 227, 5, 39195, 12, 1208, 36, 112, 4, 246, 12, 180, 4839, 8, 25664, 12, 1208, 27185, 2192, 479, 1437, 50118, 5, 39195, 12, 1208, 36, 112, 4, 246, 12, 180, 4839, 675, 24414, 21, 67, 12333, 11, 18746, 9, 5, 3222, 11181, 33267, 17512, 882, 2156, 5473, 1075, 44650, 1940, 16068, 118, 3876, 1809, 636, 414, 8, 11, 5, 4118, 2508, 2078, 787, 45421, 1459, 479, 1437, 50118, 787, 45421, 1459, 4633, 14, 5, 976, 9, 2514, 4605, 2716, 476, 10701, 31, 39195, 12, 1208, 36, 112, 4, 246, 12, 180, 4839, 675, 7, 35794, 12, 1208, 36, 112, 4, 406, 12, 180, 4839, 675, 8, 172, 124, 7, 39195, 12, 1208, 36, 112, 4, 246, 12, 180, 4839, 479, 1437, 50118, 5, 27185, 2192, 31, 5, 22455, 787, 1178, 40051, 401, 68, 27779, 360, 36, 787, 1178, 40051, 306, 68, 27779, 107, 4839, 33, 57, 1687, 31, 13466, 479, 1437, 50118, 787, 45421, 1459, 2801, 10, 545, 4, 246, 12, 2151, 36, 40569, 12, 1208, 4839, 675, 24414, 11, 5, 3778, 27840, 1530, 8, 11, 5, 5473, 1075, 44650, 414, 479, 1437, 50118, 787, 45421, 1459, 24305, 5, 21263, 731, 9, 538, 34968, 148, 4118, 16726, 753, 479, 1437, 50118, 51, 303, 10, 504, 12, 2151, 36, 31366, 12, 1208, 4839, 675, 24414, 11, 24186, 731, 9, 5, 3486, 16494, 32515, 479, 1437, 50118, 787, 45421, 1459, 1474, 42, 898, 13, 5, 787, 1178, 40051, 406, 24186, 414, 13, 4118, 16726, 291, 8, 733, 8, 303, 10, 4996, 11, 5, 476, 19416, 763, 583, 195, 15274, 1749, 360, 479, 1437, 50118, 787, 45421, 1459, 303, 10, 601, 12, 2151, 36, 30703, 12, 1208, 4839, 675, 24414, 9, 3778, 27840, 1134, 8, 49, 911, 31, 15077, 7, 11265, 479, 1437, 50118, 209, 7601, 4633, 14, 5, 5933, 9, 42, 675, 16, 15594, 8, 5, 1219, 9, 42, 675, 24414, 16, 202, 45, 6238, 479, 1437, 50118, 787, 45421, 1459, 8, 2055, 787, 45421, 1459, 4756, 27697, 1233, 21414, 9, 476, 23, 198, 26498, 360, 13, 1230, 3778, 27840, 414, 31, 37613, 111, 26873, 36, 4943, 545, 4839, 479, 11, 42, 2225, 5, 936, 9, 5, 8066, 9, 42, 675, 24414, 13, 3778, 27840, 414, 31, 4943, 545, 16, 1687, 479, 1437, 50118, 5, 1230, 3778, 27840, 911, 2156, 5, 1266, 3778, 27840, 911, 228, 512, 12344, 10134, 2156, 5, 3708, 3778, 27840, 1530, 8, 49, 22798, 2156, 61, 32, 4756, 2]\n"
     ]
    }
   ],
   "source": [
    "print(testx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = []\n",
    "for i in range(len(testx)):\n",
    "    asdf.append(list(testx[i]))\n",
    "asdf = torch.as_tensor(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 1990,   59,  ...,   32, 4756,    2],\n",
      "        [   0,  405,   16,  ...,  143,  787,    2],\n",
      "        [   0,  281,   10,  ..., 1437, 1437,    2]])\n"
     ]
    }
   ],
   "source": [
    "print(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(asdf, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>The problem of properties of short - term changes of solar activity has been considered extensively. Several periodicities were detected, but the periodicities about 155 days and from the interval of @xmath3 $ ] days are mentioned most often. The power of this periodicity started growing at cycle 19, decreased in cycles 20 and 21 and disappered after cycle 21.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>It is believed that the direct detection of gravitational waves ( gws) will bring the era of gravitational wave astronomy. Interferometer detectors are now under operation and awaiting the first signal of gws. pulsar timing arrays ( ptas) can be used to search for very low frequency gravitational waves. The promising sources are super massive black hole binaries and cosmic ( super)string.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>The tunneling through a potential barrier plays a very important role in the microscopic world and has been studied extensively since the birth of quantum mechanics. For most of the potential barriers, the penetrability can not be calculated analytically. In the present work, we derived a new barrier penetration formula based on the wkb approximation. We apply this new formula to evaluate @xmath0 decay half - lives of atomic nuclei.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge_score = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(data_val)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='test-trainer',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tok['test'],\n",
    "    eval_dataset=tok['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tok['test'][:2000]\n",
    "# samples = {k: v for k, v in samples.items() if k not in ['abstract', 'article']}\n",
    "# batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = 500\n",
    "# tok_abs = []\n",
    "\n",
    "# for i in (data_test[pos:pos + chunk] for pos in range(0, len(data_test), chunk)):\n",
    "#     tok_abs.append(tokenizer(i['abstract'], truncation=True, padding=True, return_tensors='tf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_art = []\n",
    "# for i in (data_test[pos:pos + chunk] for pos in range(0, len(data_test), chunk)):\n",
    "#     tok_art.append(tokenizer(i['article'], truncation=True, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# tok = torch.cat(tok_abs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tok_abs = tokenizer(data_train['article'], truncation=True, padding='max_length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sf2023')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91b065e1e98bd94a84b52565b1e9eeed2f9dacd7b4acfae95e9d63e228b17d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
