{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: arxiv-summarization/section\n",
      "Found cached dataset arxiv-summarization (C:/Users/JustinDu/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac146908fc3843a4a0d9668f01d89f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset['train'].shuffle(seed=42)\n",
    "data_test = dataset['test'].shuffle(seed=42)\n",
    "data_val = dataset['validation'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.shard(num_shards=6, index=0)\n",
    "data_val = data_val.shard(num_shards=24, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "checkpoint = 'facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "\n",
    "def tokenize_function(data):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        data[\"article\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        data['abstract'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313e8a6a6493402faf9d2317126ca139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31479c7868304b6981008d9e07f050e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok = data_test.map(tokenize_function, batched=True)\n",
    "tok_val = data_val.map(tokenize_function, batched=True)\n",
    "tok = tok.remove_columns(\n",
    "    data_test.column_names\n",
    ")\n",
    "tok_val = tok_val.remove_columns(\n",
    "    data_val.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JustinDu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "rouge_score = evaluate.load('rouge')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is done preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.set_format('torch')\n",
    "tok_val.set_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Trainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='arxiv-accelerate',\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JustinDu\\Documents\\GitHub\\sf2023\\BARTxiv is already a clone of https://huggingface.co/kworts/BARTxiv. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b271053863427a8fffb1c004ff39a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\accelerate\\accelerator.py:224: FutureWarning: `fp16=True` is deprecated and will be removed in version 0.15.0 of ðŸ¤— Accelerate. Use `mixed_precision='fp16'` instead.\n",
      "  warnings.warn(\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72893ed9d48d448bb9028db9c96982e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'rouge1': 39.9374998379588, 'rouge2': 13.601410726666902, 'rougeL': 21.443067586705077, 'rougeLsum': 35.79983961095115}\n",
      "Epoch 0 loss:  tensor(0.7191, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85efd9eac8cc40f1b74765b4119f1b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: {'rouge1': 41.16876106851064, 'rouge2': 14.703784631968752, 'rougeL': 22.409495597189608, 'rougeLsum': 36.935915127985524}\n",
      "Epoch 1 loss:  tensor(0.6636, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e86e3245e64c2488a1daecfb9ad658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: {'rouge1': 41.65543775840364, 'rouge2': 14.963702922887496, 'rougeL': 22.691063150059456, 'rougeLsum': 37.39672585628282}\n",
      "Epoch 2 loss:  tensor(0.5980, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c076079286484a94bab54227042f4bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: {'rouge1': 41.41129411564528, 'rouge2': 14.87800082561363, 'rougeL': 22.63183365329553, 'rougeLsum': 37.15833697989158}\n",
      "Epoch 3 loss:  tensor(0.5825, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e68ddf0a0d443ceafcba36faa6517c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: {'rouge1': 41.72649494936845, 'rouge2': 15.027706038488601, 'rougeL': 22.89953676791904, 'rougeLsum': 37.35662947169073}\n",
      "Epoch 4 loss:  tensor(0.5630, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61053c700eb4733b458a69609c95c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: {'rouge1': 41.59445622557503, 'rouge2': 14.739428209651495, 'rougeL': 22.81667528958453, 'rougeLsum': 37.076457384638296}\n",
      "Epoch 5 loss:  tensor(0.5400, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf6bcf6aa724a2a865e6731fc8b32bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: {'rouge1': 41.69963870827386, 'rouge2': 14.913768587731429, 'rougeL': 22.862963375002188, 'rougeLsum': 37.367994925166094}\n",
      "Epoch 6 loss:  tensor(0.5319, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JustinDu\\Documents\\GitHub\\sf2023\\mainAccelerate.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JustinDu/Documents/GitHub/sf2023/mainAccelerate.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m accelerator\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JustinDu/Documents/GitHub/sf2023/mainAccelerate.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m training_args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JustinDu/Documents/GitHub/sf2023/mainAccelerate.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JustinDu/Documents/GitHub/sf2023/mainAccelerate.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JustinDu/Documents/GitHub/sf2023/mainAccelerate.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\accelerate\\optimizer.py:134\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     scale_before \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    136\u001b[0m     scale_after \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:341\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    339\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 341\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[0;32m    345\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:288\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 288\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\sf2023\\lib\\site-packages\\torch_optimizer\\adafactor.py:202\u001b[0m, in \u001b[0;36mAdafactor.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    198\u001b[0m     exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2t)\u001b[39m.\u001b[39madd_(update, alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2t)\n\u001b[0;32m    199\u001b[0m     torch\u001b[39m.\u001b[39mrsqrt(exp_avg_sq, out\u001b[39m=\u001b[39mupdate)\u001b[39m.\u001b[39mmul_(grad)\n\u001b[0;32m    201\u001b[0m update\u001b[39m.\u001b[39mdiv_(\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mmax\u001b[39;49m(\u001b[39m1.0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rms(update) \u001b[39m/\u001b[39;49m group[\u001b[39m'\u001b[39;49m\u001b[39mclip_threshold\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    204\u001b[0m update\u001b[39m.\u001b[39mmul_(lr)\n\u001b[0;32m    206\u001b[0m \u001b[39mif\u001b[39;00m use_first_moment:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import socket\n",
    "import torch_optimizer as optim\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import LoggerType\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from huggingface_hub import Repository\n",
    "from transformers import get_scheduler\n",
    "\n",
    "repo = Repository('BARTxiv', clone_from='kworts/BARTxiv')\n",
    "device = torch.device('cuda')\n",
    "current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "url = current_time + \"_\" + socket.gethostname()\n",
    "writer = SummaryWriter(log_dir=f'BARTxiv/runs/{url}')\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tok, \n",
    "    batch_size=training_args.per_device_train_batch_size,\n",
    "    collate_fn=data_collator\n",
    "    )\n",
    "dataloader_val = DataLoader(\n",
    "    tok_val, \n",
    "    batch_size=training_args.per_device_train_batch_size,\n",
    "    collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "num_update_steps_per_epoch = len(dataloader)\n",
    "num_training_steps = int(num_update_steps_per_epoch * training_args.num_train_epochs / training_args.gradient_accumulation_steps)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "optimizer = optim.Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=3e-6)\n",
    "\n",
    "accelerator = Accelerator(fp16=training_args.fp16)\n",
    "model, optimizer, dataloader, dataloader_val = accelerator.prepare(model, optimizer, dataloader, dataloader_val)\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader, start=0):\n",
    "        loss = model(**batch).loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    writer.add_scalar(f'train/loss', loss.item(), epoch)\n",
    "    model.eval()\n",
    "\n",
    "    eval_bar = tqdm(range(len(dataloader_val)))\n",
    "\n",
    "    for step, batch in enumerate(dataloader_val, start=0):\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch.values())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[0],\n",
    "                attention_mask=batch[1],\n",
    "                max_new_tokens=1024,\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[2]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[2], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "            eval_bar.update(1)\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    for key, value in result.items():\n",
    "        writer.add_scalar(f'eval/{key}', value, epoch)\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "    print(f\"Epoch {epoch} loss: \", loss)\n",
    "    # accelerator.save_state('checkpoint') ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d384c668ab3d4646a472b264dc67c710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9f16b424934da1835d2f671d0962ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b99550d5194c8ab1960fc188f358e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar22_14-28-05_DESKTOP-4P0HGA6/events.out.tfevents.1679520485.DESKTOP-4P0HGA6.20104.0:   0%| â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/kworts/BARTxiv\n",
      "   31db272..2e3bbf0  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/kworts/BARTxiv\n",
      "   31db272..2e3bbf0  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/kworts/BARTxiv/commit/2e3bbf043fe0d9c6b1845a312ced0299712bcb35'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained('BARTxiv', save_function=accelerator.save)\n",
    "tokenizer.save_pretrained('BARTxiv')\n",
    "repo.git_pull()\n",
    "repo.push_to_hub(commit_message=f'Training completed ({len(tok)} examples, {epoch + 1} epochs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData, EvalResult\n",
    "\n",
    "card_data = ModelCardData(\n",
    "    language='en',\n",
    "    license='mit',\n",
    "    tags=['summarization', 'bart'],\n",
    "    datasets='ccdv/arxiv-summarization',\n",
    "    library_name='transformers',\n",
    "    eval_results=[\n",
    "        EvalResult(\n",
    "            task_type='summarization',\n",
    "            dataset_type='ccdv/arxiv-summarization',\n",
    "            dataset_name='arxiv-summarization',\n",
    "            metric_type='rouge1',\n",
    "            metric_value=result['rouge1'].item(),\n",
    "            dataset_split='validation'\n",
    "        ),\n",
    "        EvalResult(\n",
    "            task_type='summarization',\n",
    "            dataset_type='ccdv/arxiv-summarization',\n",
    "            dataset_name='arxiv-summarization',\n",
    "            metric_type='rouge2',\n",
    "            metric_value=result['rouge2'].item(),\n",
    "            dataset_split='validation'\n",
    "        ),\n",
    "    ],\n",
    "    model_name='BARTxiv',\n",
    ")\n",
    "card = ModelCard.from_template(card_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/kworts/BARTxiv/blob/main/README.md'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.push_to_hub(commit_message='update model card', repo_id='kworts/BARTxiv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d91b065e1e98bd94a84b52565b1e9eeed2f9dacd7b4acfae95e9d63e228b17d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
